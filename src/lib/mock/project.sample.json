{
  "title": "Building the GPT Tokenizer from Scratch",
  "difficulty": "Medium",
  "timeline": "1â€“2 weeks",
  "description": "This project aims to implement a Byte Pair Encoding (BPE) tokenizer from scratch, similar to those used in large language models like GPT-2 and GPT-4. The tokenizer translates raw text into sequences of numerical tokens, which are then fed into a language model for training or inference. Understanding tokenization is crucial because it's at the heart of many unexpected behaviors and limitations in large language models, such as spelling difficulties, issues with non-English languages, and arithmetic problems.",
  "jobs": [
    { "title": "Machine Learning Engineer â€” NLP (Intern/Entry)", "url": "https://example.com/jobs/ml-nlp-intern" },
    { "title": "Research Engineer â€” Tokenization/Preprocessing", "url": "https://example.com/jobs/research-engineer-tokenization" },
    { "title": "Data Engineer â€” Text Processing", "url": "https://example.com/jobs/data-engineer-text" }
  ],
  "skills": [
    "Python (intermediate)",
    "Unicode & UTF-8 fundamentals",
    "Algorithms and data structures",
    "Regular expressions",
    "Performance profiling and memory awareness",
    "Reproducible experimentation",
    "Testing and edge-case handling"
  ],
  "metadata": [
    {
      "name": "1. Character-Level Tokenization",
      "overview": "This section introduces the fundamental concept of tokenization and revisits a simple character-level tokenizer as a starting point.",
      "deliverables": [
        {
          "task": "Create a dataset",
          "file": "dataset.py",
          "spec": "Load a small corpus into a single Python string for experimentation.",
          "implementation": [
            "Read the file with UTF-8 and normalize newlines.",
            "Expose a get_text() helper that returns the raw string."
          ],
          "code": "# Load dataset\nwith open('data.txt', 'r', encoding='utf-8') as f:\n    text = f.read()\n\n# Normalize newlines\ntext = text.replace('\\r\\n', '\\n')\n\ndef get_text():\n    return text"
        },
        {
          "task": "Create vocbaulary",
          "file": "char_vocab.py",
          "spec": "Build a character-level vocabulary and ID maps.",
          "implementation": [
            "Use set() over the corpus to collect unique characters.",
            "Create stoi/itos dicts and verify round-trip on a sample."
          ],
          "code": "# Build vocabulary\nchars = sorted(list(set(text)))\nstoi = {ch: i for i, ch in enumerate(chars)}\nitos = {i: ch for i, ch in enumerate(chars)}\n\n# Test round-trip\nassert itos[stoi['a']] == 'a'"
        },
        {
          "task": "Encode and Decode",
          "file": "char_tokenizer.py",
          "spec": "Encode/decode by single characters using the maps.",
          "implementation": [
            "Implement encode(text)->List[int] and decode(ids)->str.",
            "Add basic tests for empty string and ASCII/non-ASCII."
          ],
          "code": "def encode(text):\n    return [stoi[c] for c in text]\n\ndef decode(ids):\n    return ''.join([itos[i] for i in ids])\n\n# Test\nencoded = encode('hello')\ndecoded = decode(encoded)\nassert decoded == 'hello'"
        }
      ],
      "learning_materials": [
        {
          "title": "What is Tokenization?",
          "body": "The process of converting text into sequences of tokens (integers) that a large language model can understand."
        },
        {
          "title": "Character-Level Tokenization",
          "body": "A naive approach where each character in the text is treated as a separate token and mapped to a unique integer ID."
        },
        {
          "title": "Vocabulary",
          "body": "A set of all possible characters or chunks that the tokenizer can recognize."
        },
        {
          "title": "Embedding Table",
          "body": "A lookup table where each token's integer ID corresponds to a trainable vector that feeds into the Transformer model."
        }
      ]
    },
    {
      "name": "2. Unicode and UTF-8 Encoding",
      "overview": "This section delves into how computers represent text, specifically focusing on Unicode and the UTF-8 encoding scheme, which is crucial for handling diverse languages and symbols.",
      "deliverables": [
        {
          "task": "unicode_demo.py",
          "spec": "Explore code points and basic conversions.",
          "implementation": [
            "Show ord()/chr() round-trips on diverse characters.",
            "Print code points and hex representations."
          ],
          "code": "# Unicode code points\nprint(ord('A'))  # 65\nprint(chr(65))   # 'A'\nprint(hex(ord('ðŸš€')))  # Rocket emoji code point"
        },
        {
          "task": "utf8_bytes.py",
          "spec": "Exercise UTF-8 encode/decode and error handling.",
          "implementation": [
            "Implement to_bytes(text)->List[int] via encode('utf-8').",
            "Implement from_bytes(bs)->str using errors='replace'."
          ],
          "code": "def to_bytes(text):\n    return list(text.encode('utf-8'))\n\ndef from_bytes(bs):\n    return bytes(bs).decode('utf-8', errors='replace')\n\n# Test\ntext = 'Hello ðŸš€'\nbytes_list = to_bytes(text)\nback_to_text = from_bytes(bytes_list)\nprint(f'Original: {text}')\nprint(f'Bytes: {bytes_list}')\nprint(f'Decoded: {back_to_text}')"
        }
      ],
      "learning_materials": [
        {
          "title": "Unicode Code Points",
          "body": "A standard that defines roughly 150,000 characters across 161 scripts, each represented by a unique integer (code point)."
        },
        {
          "title": "Encodings (UTF-8, UTF-16, UTF-32)",
          "body": "Methods to translate Unicode text into binary data (byte streams)."
        },
        {
          "title": "UTF-8",
          "body": "The most common encoding, which is a variable-length encoding where each Unicode code point translates to 1 to 4 bytes. It's also backward compatible with ASCII."
        },
        {
          "title": "Limitations of Raw Code Points/Bytes as Tokens",
          "body": "Using raw Unicode code points would result in a very large and unstable vocabulary (150,000+ items). Using raw UTF-8 bytes (256 possible values) would lead to extremely long token sequences, making the Transformer inefficient due to limited context length."
        }
      ]
    },
    {
      "name": "3. Byte Pair Encoding Algorithm",
      "overview": "This is the core of the tokenizer, where the BPE algorithm is implemented to compress byte sequences into a tunable vocabulary of tokens.",
      "deliverables": [
        {
          "task": "bpe_core.py",
          "spec": "Implement get_stats and merge primitives over integer token streams.",
          "implementation": [
            "Represent sequences as lists of ints (initially bytes).",
            "get_stats: count adjacent pairs into a dict[(a,b)]=freq.",
            "merge: replace pair occurrences with a new token id."
          ],
          "code": "def get_stats(ids):\n    counts = {}\n    for pair in zip(ids, ids[1:]):\n        counts[pair] = counts.get(pair, 0) + 1\n    return counts\n\ndef merge(ids, pair, idx):\n    newids = []\n    i = 0\n    while i < len(ids):\n        if i < len(ids) - 1 and ids[i] == pair[0] and ids[i + 1] == pair[1]:\n            newids.append(idx)\n            i += 2\n        else:\n            newids.append(ids[i])\n            i += 1\n    return newids"
        },
        {
          "task": "bpe_train.py",
          "spec": "Iterative merge loop to reach target vocab size.",
          "implementation": [
            "Initialize vocab with bytes 0..255; merges={}.",
            "Repeat: pick most frequent pair via max(..., key=freq).",
            "Assign new id; update sequences; record merges in order."
          ],
          "code": "# Initialize vocabulary\nvocab_size = 256\nnum_merges = vocab_size - 256\nids = list(text.encode('utf-8'))\n\nmerges = {}\nvocab = {idx: bytes([idx]) for idx in range(vocab_size)}\n\nfor i in range(num_merges):\n    stats = get_stats(ids)\n    pair = max(stats, key=stats.get)\n    idx = vocab_size + i\n    ids = merge(ids, pair, idx)\n    merges[pair] = idx\n    vocab[idx] = vocab[pair[0]] + vocab[pair[1]]"
        },
        {
          "task": "bpe_io.py",
          "spec": "Persist vocab and merges for reuse.",
          "implementation": [
            "Save/load merges list (ordered) and id->byte mapping.",
            "Ensure deterministic serialization for reproducibility."
          ],
          "code": "import pickle\n\ndef save_vocab(vocab, merges, file_prefix):\n    with open(f'{file_prefix}_vocab.pkl', 'wb') as f:\n        pickle.dump(vocab, f)\n    with open(f'{file_prefix}_merges.pkl', 'wb') as f:\n        pickle.dump(merges, f)\n\ndef load_vocab(file_prefix):\n    with open(f'{file_prefix}_vocab.pkl', 'rb') as f:\n        vocab = pickle.load(f)\n    with open(f'{file_prefix}_merges.pkl', 'rb') as f:\n        merges = pickle.load(f)\n    return vocab, merges"
        }
      ],
      "learning_materials": [
        {
          "title": "BPE Algorithm",
          "body": "An iterative compression algorithm that finds the most frequently occurring pair of tokens in a sequence and replaces them with a new, single token, which is then added to the vocabulary."
        },
        {
          "title": "Vocabulary Expansion",
          "body": "As merges occur, new tokens are minted, increasing the vocabulary size and reducing the sequence length."
        },
        {
          "title": "Hyperparameter Tuning",
          "body": "The final vocabulary size is a hyperparameter that needs to be tuned to find a balance between sequence length and embedding table size."
        }
      ]
    },
    {
      "name": "4. Encoding and Decoding",
      "overview": "This section focuses on using the trained merges and vocab to convert raw text into token sequences (encoding) and token sequences back into raw text (decoding).",
      "deliverables": [
        {
          "task": "codec.py",
          "spec": "Encode text to token ids and decode ids to text using trained merges.",
          "implementation": [
            "encode: text -> utf-8 bytes -> iterative merges (in order).",
            "decode: ids -> concatenate byte chunks -> bytes.decode('utf-8').",
            "Cover edge cases: empty text, unknown bytes."
          ],
          "code": "def encode(text):\n    # Convert to bytes\n    tokens = list(text.encode('utf-8'))\n    \n    # Apply merges in order\n    while len(tokens) > 1:\n        stats = get_stats(tokens)\n        if not stats:\n            break\n        \n        # Find most frequent pair\n        pair = max(stats, key=stats.get)\n        if pair not in merges:\n            break\n        \n        # Apply merge\n        tokens = merge(tokens, pair, merges[pair])\n    \n    return tokens\n\ndef decode(ids):\n    # Convert token ids back to bytes\n    bytes_list = []\n    for idx in ids:\n        bytes_list.extend(vocab[idx])\n    \n    return bytes(bytes_list).decode('utf-8', errors='replace')"
        },
        {
          "task": "tests_codec.py",
          "spec": "Round-trip tests for encode/decode.",
          "implementation": [
            "Assert decode(encode(x))==x for ASCII and multi-byte scripts.",
            "Test stability under trailing whitespace."
          ],
          "code": "# Test round-trip\ndef test_roundtrip(text):\n    encoded = encode(text)\n    decoded = decode(encoded)\n    assert decoded == text, f'Expected {text!r}, got {decoded!r}'\n\n# Test various cases\ntest_roundtrip('hello')\ntest_roundtrip('Hello world!')\ntest_roundtrip('ðŸš€ Python 3.9')\ntest_roundtrip('')"
        }
      ],
      "learning_materials": [
        {
          "title": "Decoding",
          "body": "Given a sequence of token IDs, reconstruct the original byte sequence by looking up each token's byte representation in the vocab and concatenating them, then decoding the resulting bytes into a string."
        },
        {
          "title": "Encoding",
          "body": "Given a raw text string, first encode it into UTF-8 bytes. Then, iteratively apply the merges in the correct order (from earliest to latest) to compress the byte sequence into the final token sequence."
        },
        {
          "title": "Order of Merges",
          "body": "Merges must be applied in the same order they were created during training to ensure consistent encoding."
        }
      ]
    },
    {
      "name": "5. Advanced Considerations (GPT-2/GPT-4)",
      "overview": "This section explores the complexities and specific design choices made in state-of-the-art tokenizers like those used in GPT-2 and GPT-4, including regular expressions for pre-splitting text and special tokens.",
      "deliverables": [
        {
          "task": "regex_pretokenize.py",
          "spec": "Apply GPT-2â€“style pre-splitting before BPE merges.",
          "implementation": [
            "Compile the complex regex and call re.findall() over text.",
            "Prevent merges across categories per pattern semantics."
          ],
          "code": "import re\n\n# GPT-2 pre-tokenization regex\ngpt2_pattern = re.compile(r\"(?i)'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\")\n\ndef pretokenize(text):\n    return gpt2_pattern.findall(text)\n\n# Test\ntext = \"Hello, world! How are you?\"\ntokens = pretokenize(text)\nprint(tokens)"
        },
        {
          "task": "special_tokens.py",
          "spec": "Register and reserve special tokens outside BPE.",
          "implementation": [
            "Maintain a reserved id range and mapping for special tokens.",
            "Document effects on embedding/LM heads when adding tokens."
          ],
          "code": "# Special tokens for chat\nspecial_tokens = {\n    '<|endoftext|>': 50256,\n    '<|im_start|>': 50257,\n    '<|im_end|>': 50258,\n}\n\n# When adding new tokens, extend vocab\nvocab_size = 50257  # GPT-2 vocab size\nnew_tokens = ['<|user|>', '<|assistant|>']\nfor i, token in enumerate(new_tokens):\n    special_tokens[token] = vocab_size + i"
        }
      ],
      "learning_materials": [
        {
          "title": "Pre-splitting Text with Regular Expressions",
          "body": "Tokenizers like GPT-2 use complex regular expressions to split text into chunks before applying BPE. This prevents merges across certain categories (e.g., letters and punctuation, numbers and letters) to avoid suboptimal tokenization."
        },
        {
          "title": "Case Sensitivity",
          "body": "Differences in how uppercase and lowercase characters are handled can lead to inconsistent tokenization if not addressed (e.g., re.IGNORECASE flag in regex)."
        },
        {
          "title": "Whitespace Handling",
          "body": "Specific rules for how spaces are grouped or separated, especially in code, can significantly impact tokenization efficiency and model performance."
        },
        {
          "title": "Special Tokens",
          "body": "Dedicated tokens (e.g., <|endoftext|>, <|im_start|>) are used to delimit documents, conversations, or introduce special functionality. These are handled outside the standard BPE algorithm."
        },
        {
          "title": "Model Surgery for Special Tokens",
          "body": "Adding new special tokens requires extending the embedding matrix and the final linear layer of the Transformer model to accommodate the new vocabulary elements."
        }
      ]
    },
    {
      "name": "6. Tokenizer Libraries and Design Choices",
      "overview": "This section compares two prominent tokenizer libraries, OpenAI's Tiktoken and Google's SentencePiece, highlighting their different approaches and design philosophies.",
      "deliverables": [
        {
          "task": "notes_tiktoken.md",
          "spec": "Document practical usage patterns and quirks of tiktoken.",
          "implementation": [
            "Record examples of encode/decode for tricky inputs.",
            "Note pretokenization regex and byte-level behavior."
          ],
          "code": "# Tiktoken usage\nimport tiktoken\n\n# Load encoder\nenc = tiktoken.get_encoding('gpt2')\n\n# Encode/decode\ntext = 'Hello, world!'\ntokens = enc.encode(text)\nback = enc.decode(tokens)\nprint(f'Tokens: {tokens}')\nprint(f'Decoded: {back}')"
        },
        {
          "task": "spm_train.md",
          "spec": "Recipe for training SentencePiece with byte_fallback and prefixes.",
          "implementation": [
            "Show trainer CLI/ Python API examples.",
            "Compare vocab sizes and tokenization density."
          ],
          "code": "# SentencePiece training\n# CLI: spm_train --input=data.txt --model_prefix=sp --vocab_size=1000 --byte_fallback\n\n# Python API\nfrom sentencepiece import SentencePieceProcessor\n\nsp = SentencePieceProcessor(model_file='sp.model')\ntokens = sp.encode('Hello world')\nprint(tokens)"
        }
      ],
      "learning_materials": [
        {
          "title": "Tiktoken (OpenAI)",
          "body": "Primarily an inference library, with training code not publicly released for GPT-2/GPT-4 tokenizers. Applies BPE on the byte level (UTF-8 bytes). Uses a pre-splitting regex pattern to enforce merging rules."
        },
        {
          "title": "SentencePiece (Google)",
          "body": "Supports both training and inference and is used by models like Llama and Mistral. Applies BPE directly on Unicode code points. Features 'byte fallback' for rare code points."
        }
      ]
    },
    {
      "name": "7. Impact on LLM Behavior",
      "overview": "This section revisits various \"weird\" behaviors of large language models and explains how they fundamentally trace back to tokenization choices.",
      "deliverables": [
        {
          "task": "analysis_notes.md",
          "spec": "Collect examples demonstrating tokenization-driven quirks.",
          "implementation": [
            "Record token counts across languages and formats (JSON/YAML).",
            "Include edge cases: trailing space, special tokens, large numbers."
          ],
          "code": "# Tokenization analysis\nimport tiktoken\n\nenc = tiktoken.get_encoding('gpt2')\n\n# Compare languages\ntest_cases = [\n    'Hello world',\n    'Bonjour le monde',\n    'ä½ å¥½ä¸–ç•Œ',\n    'ðŸš€ Python 3.9'\n]\n\nfor text in test_cases:\n    tokens = enc.encode(text)\n    print(f'{text!r} -> {len(tokens)} tokens: {tokens}')"
        }
      ],
      "learning_materials": [
        {
          "title": "Spelling and Character-Level Tasks",
          "body": "LLMs struggle with tasks like spelling or reversing strings because tokens can be long chunks of characters, and the model doesn't process individual characters within a token."
        },
        {
          "title": "Non-English Languages",
          "body": "Non-English text often tokenizes into significantly more tokens than equivalent English text, leading to 'bloated' sequences that consume more of the Transformer's finite context length and reduce performance."
        },
        {
          "title": "Arithmetic",
          "body": "The arbitrary tokenization of numbers (e.g., a four-digit number might be one, two, or more tokens) makes it difficult for LLMs to perform character-level arithmetic operations consistently."
        },
        {
          "title": "Code Efficiency",
          "body": "Inefficient tokenization of whitespace and indentation in code can dramatically increase sequence length, reducing the effective context window for the model and hindering its coding ability."
        }
      ]
    }
  ]
}
